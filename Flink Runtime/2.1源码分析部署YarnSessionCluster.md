

## 前言

这个flink作业部署到yarn上，源码分析的第一篇。

## 提交作业时

./step2_submit_job.sh  application_1530686317259_3534169

2018-08-04 19:45:36



### CliFrontend



CliFrontend.main:
-> CliFrontend cli = new CliFrontend(configuration, customCommandLines);
-> cli.parseParameters(args)
--> run() -> runProgram方法的主要逻辑如下

```java
/*
Create a {@link ClusterDescriptor} from the given configuration, 
configuration directory and the command line.
*/
final ClusterDescriptor<T> clusterDescriptor = customCommandLine.createClusterDescriptor(commandLine);

/*Returns the cluster id if a cluster id was specified on the command line, otherwise it returns null.
A cluster id identifies a running cluster, 
e.g. the Yarn application id for a Flink cluster running on Yarn. */
final T clusterId = customCommandLine.getClusterId(commandLine);

final ClusterClient<T> client;

// directly deploy the job if the cluster is started in job mode and detached
if (isNewMode && clusterId == null && runOptions.getDetachedMode()) {
	int parallelism = runOptions.getParallelism() == -1 ? defaultParallelism : runOptions.getParallelism();
/*
Creates a {@link JobGraph} from the given {@link PackagedProgram}.
*/
	final JobGraph jobGraph = PackagedProgramUtils.createJobGraph(program, configuration, parallelism);

	final ClusterSpecification clusterSpecification = customCommandLine.getClusterSpecification(commandLine);
/*
Deploys a per-job cluster with the given job on the cluster.
*/
	client = clusterDescriptor.deployJobCluster(
		clusterSpecification,
		jobGraph,
		runOptions.getDetachedMode());

```

### build JobGraph

JobGraph jobGraph = PackagedProgramUtils.createJobGraph(program, configuration, parallelism);





## AppMaster向ResourceManager申请TM容器资源

- yarn源码分析（五） 向ResourceManager请求资源  <https://zhuanlan.zhihu.com/p/33644020>
- Flink yarn-session YarnTaskManager启动（一） <https://zhuanlan.zhihu.com/p/35743696>
- Flink yarn-session YarnTaskManager启动（二）<https://zhuanlan.zhihu.com/p/36358901

主要的流程有下面一些：

1、create a YarnClusterDescriptor  (里面new了一个yarnclient)

代码点：final ClusterDescriptor<T> clusterDescriptor = customCommandLine.createClusterDescriptor(commandLine);

这个customCommandLine 是FlinkYarnSessionCli

2、create YarnClusterClient 通过YarnClusterDescriptor里面的yarnclient

代码点：clusterDescriptor.deploySessionCluster(clusterSpecification)

3、submitjob use YarnClusterClient(里面的actorSystemLoader) to (deploy job & upload jars & files)



flink  on yarn目前的架构不管是 `Flink YARN Session` 还是`Run a single Flink job on YARN` 都是会先起一个

SessionCluster的；

```
	private <T> void runProgram(
			CustomCommandLine<T> customCommandLine,
			CommandLine commandLine,
			RunOptions runOptions,
			PackagedProgram program) throws ProgramInvocationException, FlinkException {
		final ClusterDescriptor<T> clusterDescriptor = customCommandLine.createClusterDescriptor(commandLine);

		try {
			final T clusterId = customCommandLine.getClusterId(commandLine);

			final ClusterClient<T> client;

			// directly deploy the job if the cluster is started in job mode and detached
			if (isNewMode && clusterId == null && runOptions.getDetachedMode()) {
				int parallelism = runOptions.getParallelism() == -1 ? defaultParallelism : runOptions.getParallelism();

				final JobGraph jobGraph = PackagedProgramUtils.createJobGraph(program, configuration, parallelism);

				final ClusterSpecification clusterSpecification = customCommandLine.getClusterSpecification(commandLine);
				client = clusterDescriptor.deployJobCluster(
					clusterSpecification,
					jobGraph,
					runOptions.getDetachedMode());

				logAndSysout("Job has been submitted with JobID " + jobGraph.getJobID());

				try {
					client.shutdown();
				} catch (Exception e) {
					LOG.info("Could not properly shut down the client.", e);
				}
			} else {
				if (clusterId != null) {
					client = clusterDescriptor.retrieve(clusterId);
				} else {
					// also in job mode we have to deploy a session cluster because the job
					// might consist of multiple parts (e.g. when using collect)
					final ClusterSpecification clusterSpecification = customCommandLine.getClusterSpecification(commandLine);
					client = clusterDescriptor.deploySessionCluster(clusterSpecification);
				}

				try {
					client.setPrintStatusDuringExecution(runOptions.getStdoutLogging());
					client.setDetached(runOptions.getDetachedMode());
					LOG.debug("Client slots is set to {}", client.getMaxSlots());

					LOG.debug("{}", runOptions.getSavepointRestoreSettings());

					int userParallelism = runOptions.getParallelism();
					LOG.debug("User parallelism is set to {}", userParallelism);
					if (client.getMaxSlots() != MAX_SLOTS_UNKNOWN && userParallelism == -1) {
						logAndSysout("Using the parallelism provided by the remote cluster ("
							+ client.getMaxSlots() + "). "
							+ "To use another parallelism, set it at the ./bin/flink client.");
						userParallelism = client.getMaxSlots();
					} else if (ExecutionConfig.PARALLELISM_DEFAULT == userParallelism) {
						userParallelism = defaultParallelism;
					}

					executeProgram(program, client, userParallelism);
				} finally {
					if (clusterId == null && !client.isDetached()) {
						// terminate the cluster only if we have started it before and if it's not detached
						try {
							client.shutDownCluster();
						} catch (final Exception e) {
							LOG.info("Could not properly terminate the Flink cluster.", e);
						}
					}

					try {
						client.shutdown();
					} catch (Exception e) {
						LOG.info("Could not properly shut down the client.", e);
					}
				}
			}
		} finally {
			try {
				clusterDescriptor.close();
			} catch (Exception e) {
				LOG.info("Could not properly close the cluster descriptor.", e);
			}
		}
	}
```

flink版本：flink 1.6  93bb127

## 前言

采用flink yarn session部署模式去部分一个作业，通过观察启动日志和源码(其中日志开成debug模式)，

下文分以下阶段进行解读：

1、`yarn-session.sh`启动时，

2、`submit_job`提交作业时，作业任务调度日志：16：12分

## 准备工作

1、装用hadoop-client的发布机

2、从源码编译flink 1.6,（如果你们hadoop集群使用的是自己维护的hadoop版本，编译需要指定hadoop版本）

3、按需修改`flink-conf.yaml`文件配置

4、修改log4j配置文件`log4j.properties`:

```
log4j.rootLogger=DEBUG, file

log4j.appender.file.layout.ConversionPattern=%d %5p (%F:%L:%M) - %m%n
```

`log4j-yarn-session.properties`

```
log4j.rootLogger=DEBUG, stdout, file

# Log all infos in the given file
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %5p (%F:%L:%M) - %m%n

# Log all infos in the given file
log4j.appender.file=org.apache.log4j.FileAppender
log4j.appender.file.file=${log.file}-session
log4j.appender.file.append=false
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.ConversionPattern=%d %5p (%F:%L:%M) - %m%n
```



5、编写一个demo，我手写了一个消费kafka topic然后做窗口聚合的作业

6、按需写两个脚本`step1_create_session.sh`, `step2_submit_job.sh`



## 启动一个Flink YARN Session

官方文档上提供了两种方式去部署flink作业到yarn上运行，由于本文不讨论Flink作业任务的调度部分，仅仅是探究flink的JM/TM节点是怎么从yarn集群上起来的，所以接下来我会深入分析“Start Flink Session”中，执行eg.  ``./bin/yarn-session.sh -qu root.rt.flink-queue -n 4 -tm 8192 -s 4`` 脚本背后发生的事情。

`yarn-session.sh`脚本的详细的参数说明见<https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/yarn_setup.html#start-a-session>

04-08-18 16:56:40 

## FlinkYarnSessionCli

执行`yarn-session.sh`脚本之后，进入FlinkYarnSessionCli.main方法，通过日志观察它做了哪些事情：

```

```

以下是根据日志，详细翻代码的过程：

FlinkYarnSessionCli.main begin--

从flink-con.yaml读出所有的配置，然后new FlinkYarnSessionCli对象cli

FlinkYarnSessionCli构造方法里面干的事情：

```
参数解析模板配置，后面run方法会用到
// try loading a potential yarn properties file
this.yarnPropertiesFileLocation = configuration.getString(YarnConfigOptions.PROPERTIES_FILE_LOCATION);
if (yarnPropertiesLocation.exists()) {
上一次有如果人在这台机器上起了session, 就会把applicationID以及动态配置写入一个文件yarn.properties-file.location，
如果这个文件存在，后面提交作业时都是使用这个session了，也就是作业都会部署到一个集群上去。
	ApplicationId yarnApplicationIdFromYarnProperties = application信息存下来。
} else {
  yarnApplicationIdFromYarnProperties = null;
}
yarn.properties-file.location文件内容如下：
#Sat Aug 04 17:12:56 CST 2018
parallelism=8
dynamicPropertiesString=metrics.reporter.kafka.jobName\=flink_session_160@@env.java.opts\=-XX\:+UnlockCommercialFeatures -XX\:+FlightRecorder@@yarn.container-start-command-template\=/usr/local/jdk1.8.0_112/bin/java %jvmmem% %jvmopts% %logging% %class% %args% %redirects%
applicationID=application_1530686550467_3531247

YarnConfiguration yarnConfiguration =new YarnConfiguration();
```

## FlinkYarnSessionCli.run

然后执行cli.run(args)方法 //args参数包括 `-j "$FLINK_LIB_DIR"/flink-dist*.jar "$@"` , 其中`$@`是跟在yarn-session.sh命令后面的所有参数。

run方法里面的主要逻辑

```
	//line 578
	public int run(String[] args) throws CliArgsException, FlinkException {
		//命令行传入参数解析部分
		final CommandLine cmd = parseCommandLineOptions(args, true);

		if (cmd.hasOption(help.getOpt())) {
			printUsage();
			return 0;
		}

		final AbstractYarnClusterDescriptor yarnClusterDescriptor = createClusterDescriptor(cmd);

		try {
			// Query cluster for metrics
			if (cmd.hasOption(query.getOpt())) {
				final String description = yarnClusterDescriptor.getClusterDescription();
				System.out.println(description);
				return 0;
			} else {
				final ClusterClient<ApplicationId> clusterClient;
				final ApplicationId yarnApplicationId;

				if (cmd.hasOption(applicationId.getOpt())) {
					yarnApplicationId = ConverterUtils.toApplicationId(cmd.getOptionValue(applicationId.getOpt()));

					clusterClient = yarnClusterDescriptor.retrieve(yarnApplicationId);
				} else {
					final ClusterSpecification clusterSpecification = getClusterSpecification(cmd);

					clusterClient = yarnClusterDescriptor.deploySessionCluster(clusterSpecification);

					//------------------ ClusterClient deployed, handle connection details
					yarnApplicationId = clusterClient.getClusterId();

					try {
						final LeaderConnectionInfo connectionInfo = clusterClient.getClusterConnectionInfo();

						System.out.println("Flink JobManager is now running on " + connectionInfo.getHostname() +
							':' + connectionInfo.getPort() + " with leader id " + connectionInfo.getLeaderSessionID() + '.');
						System.out.println("JobManager Web Interface: " + clusterClient.getWebInterfaceURL());

						writeYarnPropertiesFile(
							yarnApplicationId,
							clusterSpecification.getNumberTaskManagers() * clusterSpecification.getSlotsPerTaskManager(),
							yarnClusterDescriptor.getDynamicPropertiesEncoded());
					} catch (Exception e) {
						try {
							clusterClient.shutdown();
						} catch (Exception ex) {
							LOG.info("Could not properly shutdown cluster client.", ex);
						}

						try {
							yarnClusterDescriptor.killCluster(yarnApplicationId);
						} catch (FlinkException fe) {
							LOG.info("Could not properly terminate the Flink cluster.", fe);
						}

						throw new FlinkException("Could not write the Yarn connection information.", e);
					}
				}

				if (yarnClusterDescriptor.isDetachedMode()) {
					LOG.info("The Flink YARN client has been started in detached mode. In order to stop " +
						"Flink on YARN, use the following command or a YARN web interface to stop it:\n" +
						"yarn application -kill " + yarnApplicationId);
				} else {
					ScheduledExecutorService scheduledExecutorService = Executors.newSingleThreadScheduledExecutor();

					final YarnApplicationStatusMonitor yarnApplicationStatusMonitor = new YarnApplicationStatusMonitor(
						yarnClusterDescriptor.getYarnClient(),
						yarnApplicationId,
						new ScheduledExecutorServiceAdapter(scheduledExecutorService));

					try {
						runInteractiveCli(
							clusterClient,
							yarnApplicationStatusMonitor,
							acceptInteractiveInput);
					} finally {
						try {
							yarnApplicationStatusMonitor.close();
						} catch (Exception e) {
							LOG.info("Could not properly close the Yarn application status monitor.", e);
						}

						clusterClient.shutDownCluster();

						try {
							clusterClient.shutdown();
						} catch (Exception e) {
							LOG.info("Could not properly shutdown cluster client.", e);
						}

						// shut down the scheduled executor service
						ExecutorUtils.gracefulShutdown(
							1000L,
							TimeUnit.MILLISECONDS,
							scheduledExecutorService);

						deleteYarnPropertiesFile();

						ApplicationReport applicationReport;

						try {
							applicationReport = yarnClusterDescriptor
								.getYarnClient()
								.getApplicationReport(yarnApplicationId);
						} catch (YarnException | IOException e) {
							LOG.info("Could not log the final application report.", e);
							applicationReport = null;
						}

						if (applicationReport != null) {
							logFinalApplicationReport(applicationReport);
						}
					}
				}
			}
		} finally {
			try {
				yarnClusterDescriptor.close();
			} catch (Exception e) {
				LOG.info("Could not properly close the yarn cluster descriptor.", e);
			}
		}

		return 0;
	}
```

 ---end FlinkYarnSessionCli.main 

## YarnClusterDescriptor

代码关键点：FlinkYarnSessionCli.createClusterDescriptor , getClusterDescriptor

这个方法里面做的事情数据创建一个YarnClusterDescriptor对象，补全里面的所有字段信息：

在YarnClusterDescriptor的构造方法里面会塞一个YarnClient

```
// line979, FlinkYarnSessionCli.getClusterDescriptor

final YarnClient yarnClient = YarnClient.createYarnClient();
yarnClient.init(yarnConfiguration); //这个yarnConfiguration从上层传过来就是空的。
yarnClient.start(); 
```

YarnClusterDescriptor类结构分析：

```
主要字段：
	private final YarnConfiguration yarnConfiguration;
	private final YarnClient yarnClient;
	/** True if the descriptor must not shut down the YarnClient. */
	private final boolean sharedYarnClient;
	private String yarnQueue;
	private String configurationDirectory;
	private Path flinkJarPath;
	private String dynamicPropertiesEncoded;
	/** Lazily initialized list of files to ship. */
	protected List<File> shipFiles = new LinkedList<>();
	private final Configuration flinkConfiguration;
	private boolean detached;
	private String customName;
	private String zookeeperNamespace;
	private String nodeLabel;
	/** Optional Jar file to include in the system class loader of all application nodes
	 * (for per-job submission). */
	private final Set<File> userJarFiles = new HashSet<>();
	private YarnConfigOptions.UserJarInclusion userJarInclusion;
主要方法：
ClusterClient<ApplicationId> retrieve(ApplicationId applicationId) //Retrieves an existing Flink Cluster.

ClusterClient<T> deploySessionCluster(ClusterSpecification clusterSpecification) //Triggers deployment of a cluster.

ClusterClient<T> deployJobCluster(
		final ClusterSpecification clusterSpecification,
		final JobGraph jobGraph,
		final boolean detached) 
		//Deploys a per-job cluster with the given job on the cluster.
		
	void killCluster(T clusterId) //Terminates the cluster with the given cluster id.
	
```



## YarnClusterDescriptor.deploySessionCluster(clusterSpecification,jobGraph,detached)

触发调用点：在FlinkYarnSessionCli.run的逻辑里面，如果是第一创建session，这个applicationId是不存在的：

方法里面实际是调用这个`deployInternal(clusterSpecification,"Flink per-job cluster",getYarnJobClusterEntrypoint(),jobGraph,detached)`方法。创建一个YarnClusterClient.

主要流程如下：

1. Check if configuration is valid
2. Check if the specified queue exists
3. Add dynamic properties to local flinkConfiguraton
4. Check if the YARN ClusterClient has the requested resources
   1. Create application via yarnClient； 
   2. validateClusterResources主要是检查Yarn的NodeManagers上有没有足够的内存来启动JMs/TMs.
5. start an application master
6. update  flinkConfiguration's `jobmanager.rpc.address|port , rest.address|port`



## 启动 application master节点

代码点：//line 696 AbstractYarnClusterDescriptor.startAppMaster

```
ApplicationReport report = startAppMaster(
	flinkConfiguration,
	applicationName,
	yarnClusterEntrypoint,
	jobGraph,
	yarnClient,
	yarnApplication,
	clusterSpecification);
```

跟进到startAppMaster方法内部，这个方法非常重要，也非常长,依次分析一下它都做了什么：

1. Initialize the file systems(HDFS)，and 添加需要上传的文件(eg. 日志配置文件、用户 jar、flink-conf.yaml， lib jars, 指定的shipFiles)
2. Add Zookeeper namespace to local flinkConfiguraton
3. add the user code jars from the provided JobGraph
4. 整理系统lib jars 与user jar的classpath顺序
5. create remotePathJar "flink.jar", <dst_hdfs_path, local_paths>
6. create remotePathConf "flink-conf.yaml", <dst_hdfs_path, local_paths>
7. create jobGraph file "job.graph" <dst_hdfs_path, local_temp_file>
8. create remoteKrb5Path
9. create remoteYarnSiteXmlPath
10. create remotePathKeytab
11. Prepare Application Master Container, `ContainerLaunchContext amContainer = 
    setupApplicationMasterContainer(yarnClusterEntrypoint,hasLogback,hasLog4j,hasKrb5,clusterSpecification.getMasterMemoryMB())`
    1. respect custom JVM options in the YAML file (javaOpts = FLINK_JVM_OPTIONS + FLINK_JM_JVM_OPTIONS)
    2. `org.apache.flink.yarn.Utils.calculateHeapSize` //TODO
    3. 配置appication maste的启动command命令，包含以下内容："%java% %jvmmem% %jvmopts% %logging% %class% %args% %redirects%" ， 其中启动类会被指定为yarnClusterEntrypoint。
12. set HDFS delegation tokens when security is enabled
    `Utils.setTokensFor(amContainer, paths, yarnConfiguration);`
13. Set `localResources` required by the amContainer. 
14. Setup CLASSPATH and environment variables for ApplicationMaster
15. set user specified app master environment variables `containerized.master.env.` in the flink-conf.yaml
16. set Flink app class path `classPathBuilder`
17. set Flink on YARN internal configuration values as environment variables used  for settings of the containers, 
    see details `org.apache.flink.yarn.YarnConfigKeys`
18. set classpath from YARN configuration, see `Utils.setupYarnClassPath(yarnConfiguration, appMasterEnv)` add `DEFAULT_YARN_APPLICATION_CLASSPATH`, like `/share/hadoop/common/*`.
19. Set up resource type requirements for ApplicationMaster(memory, vcore)
20. set  appname,type,ContainerLaunchContext, Resorce yarnqueue, yaran.tags for  ApplicationSubmissionContext `appContext`
21. add a hook to clean up in case deployment fails `Runtime.getRuntime().addShutdownHook(DeploymentFailureHook)`
22. Submitting application master `yarnClient.submitApplication(appContext)`
23. Waiting for the cluster to be allocated, until the Application State s running, then deployed successfully.
24. since deployment was successful, remove the deploymentFailureHook
25. return `ApplicationReport report = yarnClient.getApplicationReport(appId)`



## yarnclient与yarn resourcemanager通信创建application

- yarn源码分析（二）创建Application <https://zhuanlan.zhihu.com/p/32832118>
- yarn源码分析（三）AppMaster的container分配 <https://zhuanlan.zhihu.com/p/33055021>
- yarn源码分析（四）AppMaster启动 <https://zhuanlan.zhihu.com/p/33152162>



2018-08-04 17:49:04,193 DEBUG (AbstractYarnClusterDescriptor.java:1065:startAppMaster) - Application State: RUNNING
2018-08-04 17:49:04,193  INFO (AbstractYarnClusterDescriptor.java:1077:startAppMaster) - YARN application has been deployed successfully.
2018-08-04 17:49:04,193  INFO (AbstractYarnClusterDescriptor.java:1093:startAppMaster) - The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:
yarn application -kill application_1530686317259_3534169



找到container的启动类，是在AbstractYarnClusterDescriptor.setupApplicationMasterContainer中设置的，即getYarnSessionClusterEntrypoint， 接下来到这个jm的启动类YarnSessionClusterEntrypoint

