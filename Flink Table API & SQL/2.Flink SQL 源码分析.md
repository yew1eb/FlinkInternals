## 1、整体流程

新的架构中，构建抽象语法树的事情全部交给了 Calcite 去做。 + SQL query 会经过默认的 Calcite Sql Parser转变成 SQL Node tree，通过validator验证（根据schema,type,built-in）之后仍然是Sql Node Tree, 之后通过Calcite SqlToRelConverter 将SqlNode转换成RelNode关系代数表达式（也就是图中的 Logical Plan）。 + Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。 Apache Calcite是一个SQL解析与查询优化框架（这个定义是从Flink关注的视角来看，Calcite官方的定义为动态的数据管理框架）， 目前已被许多项目选择用来解析并优化SQL查询，比如：Drill、Hive、Kylin等。

我们来对上面的架构图进行解读。从中上部我们看到，可以从DataSet、DataStream以及Table Source等多种渠道来创建Table， Table相关的一些信息比如schema、数据字段及类型等信息统一被注册并存放到Calcite Catalog中。这些信息将为Table & SQL API提供元数据。接着往下看，Table API跟SQL构建的查询将被翻译成共同的逻辑计划表示，逻辑计划将作为Calcite优化器的输入。优化器结合逻辑计划以及特定的后端（DataSet、DataStream）规则进行翻译和优化，随之产生不同的计划。计划将通过代码生成器，生成特定的后端程序。后端程序的执行将返回DataSet或DataStream。
SQL 如果是SQL API 将直接用Calcite的Parser进行解释然后validate生成Calcite LogicalPlan。
(1)之后会进入优化器，利用Calcite内置的一些rule来优化LogicalPlan，也可以自己添加或者覆盖这些rule。转换成Optimized Calcite Plan后，仍然是Calcite的内部表示方式。

优化规则说明：（Flink提供了批的优化规则，和流的优化规则）这里的优化规则分为两类，

一类是Calcite提供的内置优化规则（如条件下推，剪枝等）， 再基于flink定制的一些优化rules(根据是streaming还是batch选择rulue)去优化logical Plan。 这两类规则的应用体现为下图中的①和②步骤，这两步骤都属于 Calcite 的优化阶段。

生成phsyical plan，基于flink里头的rules生成了DataStream Plan(Physical Plan) 将物Physical Plan转成Flink ExecutionPlan.

得到的DataStream Plan封装了如何将节transform成对应 DataStream/DataSet 程序的逻辑。

步骤③就是将不同的DataStream/DataSet Node通过代码生成（CodeGen），翻译成最终可执行的 DataStream/DataSet 程序。

其通过调用相应的tanslateToPlan()转换和利用CodeGen成Flink的各种算子。

(org.apache.flink.table.codegen.Compiler)

CodeGen 出的Function以字符串的形式存在。在提交任务后会分发到各个 > TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。代码生成是 Table API & SQL 中最核心的一块内容。表达式、条件、内置函数等等是需要CodeGen出具体的Function 代码的，这部分跟Spark SQL的结构很相似。



## 2、sql解析流程

SQL语句首先通过FlinkPlannerImpl.parse模块被解析为抽象语法树 SqlNode，此树称为Unresolved Logical Plan; 
Unresolved Logical Plan通过FlinkCalciteSqlValidator模块借助于数据元数据解析为LogicalRelNode， Logical Plan;

再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan; 

main:
=> Source
=> DataStream[Any]
=> SQL Query -> RelNode -> translateToPlan() -> codegen() -> function object
=> DataStream[Any] (include function) => Runtime

###  StreamTableEnvironment.sqlQuery

```
def sqlQuery(query: String): Table = {
    val planner = new FlinkPlannerImpl(getFrameworkConfig, getPlanner, getTypeFactory)
    // 1. parse the  query  sql to   calcite SqlNode
    val parsed = planner.parse(query) 
    if (null != parsed && parsed.getKind.belongsTo(SqlKind.QUERY)) {
       // 2.validate the sqNode 
        val validated = planner.validate(parsed)  //验证后的SqlNode
		   // 3. transform to a relational tree， 
  			val relational = planner.rel(validated)  //使用calcite的SqlToRelConverter转换成关系表达式，RelRoot对象(A <code>RelNode</code> is a relational expression.)
  		// LogicalRelNode  --- LogicalProject
  		//LogicalRelNode是flink自己定义的,里面塞了一个RelNode对象
		  new Table(this, LogicalRelNode(relational.rel))
		} else {
  			throw new TableException(
    			"Unsupported SQL query! sqlQuery() only accepts SQL queries of type " +
      		"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.")
		}
} 
```

  ### StreamTableEnvironment.insertInto

### StreamTableEnvironment.writeToSink

这个方法中主要调用了StreamTableEnvironment的optimze()和translate()两个方法。 负责在sql()中得到的AST tress的Optimize和生产physical plan的过程。以apeendsink为例：

```
 case appendSink: AppendStreamTableSink[_] =>
        // optimize plan, optimize过程值得深入去了解
        val optimizedPlan = optimize(table.getRelNode, updatesAsRetraction = false)
        // verify table is an insert-only (append-only) table
        if (!UpdatingPlanChecker.isAppendOnly(optimizedPlan)) {
          throw new TableException(
            "AppendStreamTableSink requires that Table has only insert changes.")
        }
        val outputType = sink.getOutputType
        val resultType = getResultType(table.getRelNode, optimizedPlan)
        // translate the Table into a DataStream and provide the type that the TableSink expects.
        val result: DataStream[T] =
          translate(
            optimizedPlan,
            resultType,
            streamQueryConfig,
            withChangeFlag = false)(outputType)
        // Give the DataStream to the TableSink to emit it.
        appendSink.asInstanceOf[AppendStreamTableSink[T]].emitDataStream(result)
```

## 3、Optimize的过程

[FLINK-6149 Separate logical and physical RelNode layer in Flink](https://issues.apache.org/jira/browse/FLINK-6066)

FlinkRelNode三个children, FlinkLogicalRel,  DataStreamRel. DataSetRel
 FlinkLogicalRel 对应生产的logical plan node
 DataStreamRel对应data stream physical plan
 DataSetRel 对应dataset physical plan

LogicalNode 貌似是TableAPI中 flink自身AST中的node，其中有toRelNode方法用于转化成calcite AST

### SteamTableEnvironment.optimize

flink sql查询优化核心逻辑

```
private[flink] def optimize(relNode: RelNode, updatesAsRetraction: Boolean): RelNode = {

    // 0. convert sub-queries before query decorrelation
    val convSubQueryPlan = runHepPlanner(
      HepMatchOrder.BOTTOM_UP, FlinkRuleSets.TABLE_SUBQUERY_RULES, relNode, relNode.getTraitSet)

    // 0. convert table references
    val fullRelNode = runHepPlanner(
      HepMatchOrder.BOTTOM_UP,
      FlinkRuleSets.TABLE_REF_RULES,
      convSubQueryPlan,
      relNode.getTraitSet)

    // 1. decorrelate
    val decorPlan = RelDecorrelator.decorrelateQuery(fullRelNode)

    // 2. convert time indicators
    val convPlan = RelTimeIndicatorConverter.convert(decorPlan, getRelBuilder.getRexBuilder)

    // 3. normalize the logical plan
    val normRuleSet = getNormRuleSet
    val normalizedPlan = if (normRuleSet.iterator().hasNext) {
      runHepPlanner(HepMatchOrder.BOTTOM_UP, normRuleSet, convPlan, convPlan.getTraitSet)
    } else {
      convPlan
    }

    // 4. optimize the logical Flink plan
    val logicalOptRuleSet = getLogicalOptRuleSet
    val logicalOutputProps = relNode.getTraitSet.replace(FlinkConventions.LOGICAL).simplify()
    val logicalPlan = if (logicalOptRuleSet.iterator().hasNext) {
      runVolcanoPlanner(logicalOptRuleSet, normalizedPlan, logicalOutputProps)
    } else {
      normalizedPlan
    }

    // 5. optimize the physical Flink plan
    val physicalOptRuleSet = getPhysicalOptRuleSet
    val physicalOutputProps = relNode.getTraitSet.replace(FlinkConventions.DATASTREAM).simplify()
    val physicalPlan = if (physicalOptRuleSet.iterator().hasNext) {
      runVolcanoPlanner(physicalOptRuleSet, logicalPlan, physicalOutputProps)
    } else {
      logicalPlan
    }

    // 6. decorate the optimized plan
    val decoRuleSet = getDecoRuleSet
    val decoratedPlan = if (decoRuleSet.iterator().hasNext) {
      val planToDecorate = if (updatesAsRetraction) {
        physicalPlan.copy(
          physicalPlan.getTraitSet.plus(new UpdateAsRetractionTrait(true)),
          physicalPlan.getInputs)
      } else {
        physicalPlan
      }
      runHepPlanner(
        HepMatchOrder.BOTTOM_UP,
        decoRuleSet,
        planToDecorate,
        planToDecorate.getTraitSet)
    } else {
      physicalPlan
    }

    decoratedPlan
  } 
```



FlinkRuleSets定义了很多rule， 用于在Optimize过程中

> DATASTREAM_NORM_RULES: 把一些节点进行normalized转换
>  LOGICAL_OPT_RULES： logical plan优化rules， 里面包含都是sql语句相关的优化(下推，剪裁)， 同时做转换：SQLRel -》FlinkLogicalRel
>  DATASTREAM_OPT_RULES: 把logical plan node转换成datastream physical
>  plan的rule， FlinkLogicalRel  -> DataStreamRel。其中的rules都继承自`calcite ConvertRule`, 而convertRule:> RelOptRule

#### **ConvertRule**

- Abstract base class for a rule which converts from one calling convention to another without changing semantics.

ConvertRule有很多继承类(比如DATASTREAM_OPT_RULES中所有，以及LOGICAL_OPT_RULES中转换flinkLogicalPlan的rules), 都实现了其convert()接口，convert()会在onMatch中调用。

ConverterRule.onMatch(RelOptRuleCall call)
 relOptRule提供了onMatch(RelOptRuleCall call) 被ConverterRule继承， 目的是当matchs() return true后被通知调用

```
public void onMatch(RelOptRuleCall call) {
 // rel 是RelOptRuleCall中rels[0] , 代表被convert的original rel
    RelNode rel = call.rel(0);
// 如果包含此trait，
    if (rel.getTraitSet().contains(inTrait)) {
      final RelNode converted = convert(rel);
      if (converted != null) {
        call.transformTo(converted);
      }
    }
  }
```

> TODO LOGICAL_OPT_RULES中实现flinkLogicalPlan的rule不光实现了convert()还实现了matchs()
>  convert()可以用debug走一遍, 看看inTrait到底是什么

Normalize Rules

```
    /**
    * RuleSet to normalize plans for stream / DataStream execution
    */
  val DATASTREAM_NORM_RULES: RuleSet = RuleSets.ofList(
    // Transform window to LogicalWindowAggregate
    DataStreamLogicalWindowAggregateRule.INSTANCE,
    WindowStartEndPropertiesRule.INSTANCE,

    // simplify expressions rules
    // 这3个rule都是从3种expresion中移除constats（用RexLiteral替换）和多余的cast,
   //（如果cast(cast date),inner的input和outer的output类型相同，就能删掉）
    ReduceExpressionsRule.FILTER_INSTANCE,
    ReduceExpressionsRule.PROJECT_INSTANCE,
    ReduceExpressionsRule.CALC_INSTANCE,

 //如果`org.apache.calcite.rel.core.Project`中包含over的expression(Projec中包含多个expressions.类似select多个column)， 那么就拆分为一个logicalProject和一个LogicalWindow（分离为windowed agg和不相干的部分）
    ProjectToWindowRule.PROJECT
  )
```

LogicalCalc -  A relational expression which computes project expressions and also filters. 它包含了`{@link LogicalProject} and {@link LogicalFilter}`的功能.

```
val LOGICAL_OPT_RULES: RuleSet = RuleSets.ofList(

// calcite内部的一些默认优化
    // convert a logical table scan to a relational expression
    TableScanRule.INSTANCE,
    EnumerableToLogicalTableScan.INSTANCE,

    // push a filter into a join
    FilterJoinRule.FILTER_ON_JOIN,
    // push filter into the children of a join
    FilterJoinRule.JOIN,
    // push filter through an aggregation
    FilterAggregateTransposeRule.INSTANCE,

    还有很多....................................................
    
 
// flink提供的优化， project和filter下推到表scan
    // scan optimization
    PushProjectIntoTableSourceScanRule.INSTANCE,
    PushFilterIntoTableSourceScanRule.INSTANCE,

    // Unnest rule
    LogicalUnnestRule.INSTANCE,

    // translate to flink logical rel nodes
    FlinkLogicalAggregate.CONVERTER,
    FlinkLogicalWindowAggregate.CONVERTER,
    FlinkLogicalOverWindow.CONVERTER,
    FlinkLogicalCalc.CONVERTER,
    FlinkLogicalCorrelate.CONVERTER,
    FlinkLogicalIntersect.CONVERTER,
    FlinkLogicalJoin.CONVERTER,
    FlinkLogicalMinus.CONVERTER,
    FlinkLogicalSort.CONVERTER,
    FlinkLogicalUnion.CONVERTER,
    FlinkLogicalValues.CONVERTER,
    FlinkLogicalTableSourceScan.CONVERTER,
    FlinkLogicalTableFunctionScan.CONVERTER,
    FlinkLogicalNativeTableScan.CONVERTER
  )
```

DATASTREAM_OPT_RULES 略, convertRule， 用于转换logical plan到DataStreamRel

DATASTREAM_DECO_RULES
 /**
 \* RuleSet to decorate plans for stream / DataStream execution
 转化为三种output表， 实现都是RelOptRule的实现
 */
 val DATASTREAM_DECO_RULES: RuleSet = RuleSets.ofList(
 // retraction rules
 DataStreamRetractionRules.DEFAULT_RETRACTION_INSTANCE,
 DataStreamRetractionRules.UPDATES_AS_RETRACTION_INSTANCE,
 DataStreamRetractionRules.ACCMODE_INSTANCE
 )

### StreamTableEnvironment.translate

translate()在TableEnvironment.writeToSink中被call，主要逻辑是将Table转换成flink可执行的代码(DataStream)


````java
  /**
    * Translates a logical [[RelNode]] into a [[DataStream]].
    *
    * @param logicalPlan The root node of the relational expression tree.
    * @param logicalType The row type of the result. Since the logicalPlan can lose the
    *                    field naming during optimization we pass the row type separately.
    * @param queryConfig     The configuration for the query to generate.
    * @param withChangeFlag Set to true to emit records with change flags.
    * @param tpe         The [[TypeInformation]] of the resulting [[DataStream]].
    * @tparam A The type of the resulting [[DataStream]].
    * @return The [[DataStream]] that corresponds to the translated [[Table]].
    */
  protected def translate[A](
      logicalPlan: RelNode,
      logicalType: RelDataType,
      queryConfig: StreamQueryConfig,
      withChangeFlag: Boolean)
      (implicit tpe: TypeInformation[A]): DataStream[A] = {

    // if no change flags are requested, verify table is an insert-only (append-only) table.
    if (!withChangeFlag && !UpdatingPlanChecker.isAppendOnly(logicalPlan)) {
      throw new TableException(
        "Table is not an append-only table. " +
        "Use the toRetractStream() in order to handle add and retract messages.")
    }

    // get CRow plan
    val plan: DataStream[CRow] = translateToCRow(logicalPlan, queryConfig)

    val rowtimeFields = logicalType
      .getFieldList.asScala
      .filter(f => FlinkTypeFactory.isRowtimeIndicatorType(f.getType))

    // convert the input type for the conversion mapper
    // the input will be changed in the OutputRowtimeProcessFunction later
    val convType = if (rowtimeFields.size > 1) {
      throw new TableException(
        s"Found more than one rowtime field: [${rowtimeFields.map(_.getName).mkString(", ")}] in " +
          s"the table that should be converted to a DataStream.\n" +
          s"Please select the rowtime field that should be used as event-time timestamp for the " +
          s"DataStream by casting all other fields to TIMESTAMP.")
    } else if (rowtimeFields.size == 1) {
      val origRowType = plan.getType.asInstanceOf[CRowTypeInfo].rowType
      val convFieldTypes = origRowType.getFieldTypes.map { t =>
        if (FlinkTypeFactory.isRowtimeIndicatorType(t)) {
          SqlTimeTypeInfo.TIMESTAMP
        } else {
          t
        }
      }
      CRowTypeInfo(new RowTypeInfo(convFieldTypes, origRowType.getFieldNames))
    } else {
      plan.getType
    }

    // convert CRow to output type
    val conversion: MapFunction[CRow, A] = if (withChangeFlag) {
      getConversionMapperWithChanges(
        convType,
        new RowSchema(logicalType),
        tpe,
        "DataStreamSinkConversion")
    } else {
      getConversionMapper(
        convType,
        new RowSchema(logicalType),
        tpe,
        "DataStreamSinkConversion")
    }

    val rootParallelism = plan.getParallelism

    val withRowtime = if (rowtimeFields.isEmpty) {
      // no rowtime field to set
      plan.map(conversion)
    } else {
      // set the only rowtime field as event-time timestamp for DataStream
      // and convert it to SQL timestamp
      plan.process(new OutputRowtimeProcessFunction[A](conversion, rowtimeFields.head.getIndex))
    }

    withRowtime
      .returns(tpe)
      .name(s"to: ${tpe.getTypeClass.getSimpleName}")
      .setParallelism(rootParallelism)
  }
````



## 与Spark SQL的区别？

一句话概述就是：借助Apache Calcite做了sql解析、逻辑树生成的过程，得到Calcite的RelRoot类，生成flink的Table，
Table里的执行计划会转化成DataSet的计算，经历物理执行计划优化等步骤。
类比Spark SQL，Calcite代替了大部分Spark SQL Catalyst的工作(Catalyst还包括了Tree/Node的定义，这部分代码Flink也’借鉴’来了)。
两者最终是计算一颗逻辑执行计划树，翻译成各自的DataSet(Spark 2.0引入Dataset并统一DataFrame，隐藏RDD到引擎内部这层，类似于执行层内部的物理执行节点)。

